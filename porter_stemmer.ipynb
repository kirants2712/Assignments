{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <CENTER>$NATIONAL$ $INSTITUTE$ $OF$ $TECHNOLOGY$ $CALICUT$\n",
    "# <CENTER>CS4041D NATURAL LANGUAGE PROCESSING</CENTER>\n",
    "## <CENTER>`ASSIGNMENT II - STOP WORDS ELIMINATION & PORTER STEMMER`\n",
    "\n",
    "### <b>KIRAN T S, B160244CS</b>\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial assignment (ie. Assignment I) on recent trends in NLP is stored in `flair.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  Natural Language Processing as a Domain has evolved over the past few years. We have got  seen several breakthroughs- ULMFiT, ELMo, BERT, among several  others. These have vastly contributed to the State-of-the-art research in NLP. currently, We are able to predict the succeeding sentence, given a sequence of preceding words. the downstream NLP tasks have depended heavily on the pre-trained word embeddings, both due to their ability to enhance learning and generalization with information learned from untagged/unlabelled information, moreover the relative ease of including them into any learning methodologies. Several recently proposed approaches transcend the initial “one word, one embedding” paradigm to better model additional features such as subword structures and meaning ambiguity. Even though having several advantages, such embeddings have the disadvantage that they can’t be accustomed to simply initialize the embedding layer of a neural network and thus require specific reworkings of the general model architecture. The challenges raised by the different word embeddings are effectively addressed by FLAIR. Developed & open-sourced by Zalando Research; FLAIR is a simple natural language processing (NLP) library/framework. PyTorch, which is used as the building block for the FLAIR’s framework, is a renowned deep learning framework. the framework provides a single, simple and unified interface for word and document embeddings which are conceptually different. The interface effectively hides all embedding-specific engineering complexity and permits researchers to “mix and match” various embeddings with very little effort to produce state of the art results. The framework additionally implements customary model training and hyperparameter selection routines. One of the interesting components of FLAIR is the inbuilt data fetching module. The module can access publicly available datasets and mold those into data structures for various experiments with ease. The framework is supplied with pre-trained models for following NLP tasks: Name-Entity Recognition (NER), Parts-of-Speech Tagging (PoS), Text Classification, sense disambiguation and classification, Training Custom Models. FLAIR runs on python and the minimum version requirement is python 3.6 in the environment. The library can be installed using pip,  the command: pip install flair.\n",
      "                                        All this appears promising but what sets FLAIR apart is that it has outperformed several state-of-the-art results in NLP including Allen AI, Bi-directional LSTMs. What provides FLAIR the edge? The solution lies within the features embedded in the FLAIR library. It contains popular and state-of-the-art word embeddings, such as GloVe, BERT, ELMo, Character Embeddings, etc. FLAIR API enables us to use all these with great ease. variant word embeddings can be combined together as a single embedding model and embed datasets using the latest FLAIR interface. This methodology of combining various embeddings has enhanced the traditional benchmarks in NLP and produced a significant rise in the F1 scores of major NLP tasks. Apart from the individual embeddings and the combination of embeddings, FLAIR also provides its own signature embedding called  ‘FLAIR Embedding’. The FLAIR embedding is characterized by its unique contextual string embeddings. Context plays a major role in predicting the next character from the knowledge learned from the previous characters and it forms the cornerstone of sequence modeling. Contextual String Embeddings leverage the internal states of a trained character language model to produce a novel type of word embedding. In simple terms, it uses certain internal principles of a trained character model, such that words can have different meanings in different sentences.\n",
      "                                     FLAIR with its contextualized representations/embeddings holds the knowledge of syntactic and semantic information that can be used to improve several downstream NLP tasks. FLAIR has set clear benchmarks in word embeddings and stacked word embeddings. These can be implemented without much hassle due to its high-level API. the future is bright for the FLAIR embedding and is continuously producing strides in the NLP domain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"flair.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## `1. StopWord Elimination`\n",
    "#### `INTRODUCTION`\n",
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words. I have compiled a list of stop words from SPACY and NLTK libraries. we'll use this list for stop word elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Stopwords in NLTK library = 179\n",
      "No of Stopwords in SPACY library = 326\n",
      "No of Stopwords in combined list = 382\n",
      "['where', 'three', 'whatever', 'its', 'until', 'have', 'not', 'side', 'that', 'who', 'shan', 'upon', 'hasn', 'seemed', \"aren't\", 'mustn', 'am', 'bottom', 'ain', 'moreover', 'elsewhere', 'don', \"won't\", 'always', 'besides', 'mostly', 'amongst', 'did', 'though', 'same', 'regarding', 'most', \"that'll\", 'must', 'whither', 'at', 'least', 'down', 'these', 'my', '’d', 'someone', 'the', 'they', 'already', 'could', 'aren', 'isn', 'mightn', \"you'll\", 'on', \"'d\", 'but', 'with', 'six', 'via', 'whereby', 'per', 'less', 'those', 'd', 'wouldn', 'her', 'across', 'their', 'seeming', '‘d', 'never', 'thereupon', 'whence', '’s', 'over', 'his', 'fifteen', 'has', 'again', 'done', \"'ll\", \"didn't\", 'something', 'toward', \"isn't\", 'myself', 'anything', 't', 'often', 'why', 'only', 'part', 'few', 'by', \"don't\", 'latterly', 're', 'your', 'once', 'needn', 'next', \"you've\", 'even', 'didn', '‘m', 'further', 'doing', 'about', 'as', \"'ve\", 'wherever', 'here', 'there', 'keep', \"should've\", 'very', \"she's\", 'give', 'since', 'won', 'namely', 'into', 'whereas', 'an', 'when', 'nobody', 'put', 'll', 'whose', 'due', 'indeed', 'we', 'me', 'beyond', 'hers', 'mine', 'make', 'through', 'empty', 'nowhere', 'thence', 'almost', 'third', \"mustn't\", 'former', 'together', 'one', 'in', 've', 'if', 'shouldn', 'of', 'ma', 'else', 'itself', 'theirs', 'using', '’ll', \"wouldn't\", 'thereby', 'without', 'be', 'hereafter', 'such', 'than', 'against', 'anyhow', 'for', 'became', 'others', 'whereupon', 'amount', 'whoever', 'herein', 'between', 'rather', 'also', \"'m\", 'used', 'however', 'eleven', 'ours', 'although', 'whole', 'back', 'hereupon', \"hadn't\", 'move', 'our', 'becomes', \"mightn't\", 'last', 'while', 'y', 'does', '’re', 'another', 'everything', 'is', 'somewhere', 'or', 'off', 'onto', 'whether', 'cannot', 'nevertheless', 'each', 'beside', 'up', 'any', 'haven', \"doesn't\", 'were', 'how', 'unless', 'serious', 'four', 'ourselves', 'themselves', 'meanwhile', 'm', 'quite', 'thus', 'twenty', 'having', \"n't\", 'wasn', 'sixty', \"'s\", 'well', 'still', 'front', 'hadn', 'anyway', 'will', 'seems', 'enough', 'just', 'everyone', 'various', '’ve', \"you're\", 'a', 'are', '‘s', 'from', \"haven't\", 'doesn', 'fifty', 'to', 'take', 'full', 'please', 'own', 'call', '‘re', 'along', \"couldn't\", 'and', 'alone', 'couldn', 'five', 'made', 'noone', 'himself', 'before', 'this', 'see', 'really', 'should', 'twelve', 'hereby', 'too', 'towards', 'throughout', '’m', 'formerly', 'below', 's', 'yours', 'them', 'may', 'everywhere', 'two', 'eight', 'nothing', \"weren't\", 'except', 'name', 'therein', 'becoming', 'nine', 'whenever', 'hundred', 'many', 'she', 'n‘t', 'after', 'no', 'which', \"shan't\", 'yourselves', 'ca', 'can', \"'re\", 'otherwise', 'thru', 'he', 'because', 'hence', 'sometime', 'yet', 'being', 'was', 'more', 'every', 'ever', 'among', 'within', 'o', \"wasn't\", 'wherein', 'other', 'you', 'might', 'anywhere', 'afterwards', \"it's\", 'had', 'around', 'first', \"hasn't\", \"needn't\", 'whom', 'been', 'become', 'behind', 'under', 'some', \"shouldn't\", 'him', 'say', 'out', 'somehow', \"you'd\", 'then', 'so', 'nor', 'several', 'much', '‘ve', 'go', '‘ll', 'anyone', 'whereafter', 'seem', 'both', 'it', 'sometimes', 'now', 'above', 'forty', 'weren', 'what', 'us', 'beforehand', 'herself', 'top', 'perhaps', 'neither', 'get', 'i', 'none', 'therefore', 'latter', 'all', 'n’t', 'thereafter', 'show', 'do', 'yourself', 'ten', 'during', 'would', 'either']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords_nltk = nltk.corpus.stopwords.words('english')\n",
    "print (\"No of Stopwords in NLTK library = \"+str(len(stopwords_nltk)))\n",
    "\n",
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_spacy\n",
    "print (\"No of Stopwords in SPACY library = \"+str(len(stopwords_spacy)))\n",
    "\n",
    "# take union of both library\n",
    "stopwords=list(set(stopwords_nltk+list(stopwords_spacy)))\n",
    "print (\"No of Stopwords in combined list = \"+str(len(stopwords)))\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output after removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing Domain evolved past years . We got seen breakthroughs- ULMFiT, ELMo, BERT, . These vastly contributed State-of-the-art research NLP . currently, We able predict succeeding sentence, given sequence preceding words . downstream NLP tasks depended heavily pre-trained word embeddings, ability enhance learning generalization information learned untagged/unlabelled information, relative ease including learning methodologies . Several recently proposed approaches transcend initial “ word, embedding ” paradigm better model additional features subword structures meaning ambiguity . Even advantages, embeddings disadvantage ’ accustomed simply initialize embedding layer neural network require specific reworkings general model architecture . The challenges raised different word embeddings effectively addressed FLAIR . Developed & open-sourced Zalando Research; FLAIR simple natural language processing (NLP) library/framework . PyTorch, building block FLAIR ’ framework, renowned deep learning framework . framework provides single, simple unified interface word document embeddings conceptually different . The interface effectively hides embedding-specific engineering complexity permits researchers “ mix match ” embeddings little effort produce state art results . The framework additionally implements customary model training hyperparameter selection routines . One interesting components FLAIR inbuilt data fetching module . The module access publicly available datasets mold data structures experiments ease . The framework supplied pre-trained models following NLP tasks: Name-Entity Recognition (NER), Parts-of-Speech Tagging (PoS), Text Classification, sense disambiguation classification, Training Custom Models . FLAIR runs python minimum version requirement python 3.6 environment . The library installed pip, command: pip install flair . All appears promising sets FLAIR apart outperformed state-of-the-art results NLP including Allen AI, Bi-directional LSTMs . What provides FLAIR edge? The solution lies features embedded FLAIR library . It contains popular state-of-the-art word embeddings, GloVe, BERT, ELMo, Character Embeddings, etc . FLAIR API enables use great ease . variant word embeddings combined single embedding model embed datasets latest FLAIR interface . This methodology combining embeddings enhanced traditional benchmarks NLP produced significant rise F1 scores major NLP tasks . Apart individual embeddings combination embeddings, FLAIR provides signature embedding called ‘ FLAIR Embedding ’ . The FLAIR embedding characterized unique contextual string embeddings . Context plays major role predicting character knowledge learned previous characters forms cornerstone sequence modeling . Contextual String Embeddings leverage internal states trained character language model produce novel type word embedding . In simple terms, uses certain internal principles trained character model, words different meanings different sentences . FLAIR contextualized representations/embeddings holds knowledge syntactic semantic information improve downstream NLP tasks . FLAIR set clear benchmarks word embeddings stacked word embeddings . These implemented hassle high-level API . future bright FLAIR embedding continuously producing strides NLP domain.\n"
     ]
    }
   ],
   "source": [
    "with open('flair.txt', 'r') as file:\n",
    "    assignment = file.read()#.replace('\\n', '')\n",
    "assignment = assignment[1:]\n",
    "\n",
    "assignment = nltk.tokenize.word_tokenize(assignment)\n",
    "assignment_v1 = [] \n",
    "\n",
    "for token in assignment:\n",
    "    if token not in stopwords:\n",
    "        assignment_v1.append(token)\n",
    "# detokenize the list \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "assignment_v2 = TreebankWordDetokenizer().detokenize(assignment_v1)\n",
    "print (assignment_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## `PORTER STEMMER`\n",
    "#### `INTRODUCTION`\n",
    "A consonant in a word is a letter other than A, E, I, O or U, and other than Y preceded by a consonant. If a letter is not a consonant it is a vowel. A consonant will be denoted by c, a vowel by v. A list ccc... of length greater than 0 will be denoted by <b>C</b>, and a list vvv... of length greater than 0 will be denoted by <b>V</b>. Any word, or part of a word, therefore has one of the four forms:\n",
    "<ul>\n",
    "<li>CVCV ... C</li>\n",
    "<li>CVCV ... V</li>\n",
    "<li>VCVC ... C</li>\n",
    "<li>VCVC ... V</li>\n",
    "</ul>\n",
    "These may all be represented by the single form\n",
    "\n",
    "> `[C]VCVC ... [V]`\n",
    "\n",
    "where the square brackets denote arbitrary presence of their contents. Using (VCm) to denote VC repeated m times, this may again be written as\n",
    "\n",
    "> `[C](VCm)[V]` \n",
    "\n",
    "m will be called the measure of any word or word part when represented in this form. The case m = 0 covers the null word.\n",
    "The rules for removing a suffix will be given in the form\n",
    "\n",
    "> `(condition) S1 -> S2`\n",
    "\n",
    "This means that if a word ends with the suffix S1, and the stem before S1 satisfies the given condition, S1 is replaced by S2. The condition is usually given in terms of m.\n",
    "The 'condition' part may also contain the following:\n",
    "\n",
    "<ul>\n",
    "<li>*S  - the stem ends with S (and similarly for the other letters).</li>\n",
    "<li>*v* - the stem contains a vowel.</li>\n",
    "<li>m=2 - TROUBLES, PRIVATE, OATEN, ORRERY.</li>\n",
    "<li>*d  - the stem ends with a double consonant (e.g. -TT, -SS).</li>\n",
    "<li>*o  - the stem ends cvc, where the second c is not W, X or Y (e.g. -WIL, -HOP)</li>    \n",
    "</ul>\n",
    "\n",
    "And the condition part may also contain expressions with and, or and not. In a set of rules written beneath each other, only one is obeyed, and this will be the one with the longest matching S1 for the given word. \n",
    "\n",
    "\n",
    "### `ALGORITHM`\n",
    "##### STEP 1a\n",
    "\n",
    "    SSES -> SS\n",
    "    IES -> I\n",
    "    SS -> SS \n",
    "    S -> \n",
    "##### STEP 1b\n",
    "\n",
    "    (m>0) EED -> EE\n",
    "    (v) ED -> \n",
    "    (v) ING -> \n",
    "    S -> \n",
    "\n",
    "If the second or third of the rules in Step 1b is successful, the following is done:\n",
    "\n",
    "    AT -> ATE \n",
    "    BL -> BLE \n",
    "    IZ -> IZE \n",
    "    S -> \n",
    "    (*d and not (*L or *S or *Z)) -> single letter \n",
    "    (m=1 and *o) -> E \n",
    "##### STEP 1c\n",
    "\n",
    "    (\\*v\\*) Y -> I \n",
    "##### STEP 2\n",
    "\n",
    "    (m>0) ATIONAL -> ATE \n",
    "    (m>0) TIONAL -> TION \n",
    "    (m>0) ENCI -> ENCE \n",
    "    (m>0) ANCI -> ANCE \n",
    "    (m>0) IZER -> IZE \n",
    "    (m>0) ABLI -> ABLE \n",
    "    (m>0) ALLI -> AL \n",
    "    (m>0) ENTLI -> ENT \n",
    "    (m>0) ELI -> E \n",
    "    (m>0) OUSLI -> OUS \n",
    "    (m>0) IZATION -> IZE \n",
    "    (m>0) ATION -> ATE \n",
    "    (m>0) ATOR -> ATE\n",
    "    (m>0) ALISM -> AL \n",
    "    (m>0) IVENESS -> IVE \n",
    "    (m>0) FULNESS -> FUL \n",
    "    (m>0) OUSNESS -> OUS \n",
    "    (m>0) ALITI -> AL \n",
    "    (m>0) IVITI -> IVE \n",
    "    (m>0) BILITI -> BLE \n",
    "##### STEP 3\n",
    "\n",
    "    (m>0) ICATE -> IC \n",
    "    (m>0) ATIVE ->\n",
    "    (m>0) ALIZE -> AL\n",
    "    (m>0) ICITI -> IC \n",
    "    (m>0) ICAL -> IC \n",
    "    (m>0) FUL -> \n",
    "    (m>0) NESS -> \n",
    "##### STEP 4\n",
    "\n",
    "    (m>1) AL -> \n",
    "    (m>1) ANCE -> \n",
    "    (m>1) ENCE -> \n",
    "    (m>1) ER -> \n",
    "    (m>1) IC -> \n",
    "    (m>1) ABLE -> \n",
    "    (m>1) IBLE -> \n",
    "    (m>1) ANT -> \n",
    "    (m>1) EMENT -> \n",
    "    (m>1) MENT -> \n",
    "    (m>1) ENT -> \n",
    "    (m>1 and (*S or *T)) ION -> \n",
    "    (m>1) OU -> \n",
    "    (m>1) ISM -> \n",
    "    (m>1) ATE -> \n",
    "    (m>1) ITI -> \n",
    "    (m>1) OUS -> \n",
    "    (m>1) IVE -> \n",
    "    (m>1) IZE -> \n",
    "##### STEP 5a\n",
    "\n",
    "    (m>1) E -> \n",
    "    (m=1 and not *o) E -> \n",
    "##### STEP 5b\n",
    "\n",
    "    (m > 1 and *d and *L) -> single letter\n",
    "\n",
    "___\n",
    "### `IMPLEMENTATION`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorterStemmer:\n",
    "    \n",
    "    #function returns true if a letter is a consonant otherwise false\n",
    "    def isCons(self, letter):#function that returns true if a letter is a consonant otherwise false\n",
    "        if letter == 'a' or letter == 'e' or letter == 'i' or letter == 'o' or letter == 'u':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    #function returns true only if the letter at i th position in the argument 'word' is a consonant.\n",
    "    def isConsonant(self, word, i):\n",
    "        letter = word[i]\n",
    "        if self.isCons(letter):\n",
    "            if letter == 'y' and self.isCons(word[i-1]):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #function returns true if the letter at i th position in the argument 'word' is a vowel\n",
    "    def isVowel(self, word, i):\n",
    "        return not(self.isConsonant(word, i))\n",
    "\n",
    "    #function returns true if the word 'stem' ends with 'letter' \n",
    "    def endsWith(self, stem, letter):\n",
    "        if stem.endswith(letter):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #function returns true if the word 'stem' contains a vowel\n",
    "    def containsVowel(self, stem):\n",
    "        for i in stem:\n",
    "            if not self.isCons(i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    #function returns true if the word 'stem' ends with 2 consonants\n",
    "    def doubleCons(self, stem):\n",
    "        if len(stem) >= 2:\n",
    "            if self.isConsonant(stem, -1) and self.isConsonant(stem, -2):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #function returns the form of the 'word' in one of the four forms.    \n",
    "    def getForm(self, word):\n",
    "        form = []\n",
    "        formStr = ''\n",
    "        for i in range(len(word)):\n",
    "            if self.isConsonant(word, i):\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'C':\n",
    "                        form.append('C')\n",
    "                else:\n",
    "                    form.append('C')\n",
    "            else:\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'V':\n",
    "                        form.append('V')\n",
    "                else:\n",
    "                    form.append('V')\n",
    "        for j in form:\n",
    "            formStr += j\n",
    "        return formStr\n",
    "\n",
    "    #function returns the value of 'm' in [C](VCm)[V]\n",
    "    def getM(self, word):\n",
    "        form = self.getForm(word)\n",
    "        m = form.count('VC')\n",
    "        return m\n",
    "\n",
    "    #function returns true if the stem ends cvc, where the second c is not W, X or Y\n",
    "    def cvc(self, word):\n",
    "        if len(word) >= 3:\n",
    "            f = -3\n",
    "            s = -2\n",
    "            t = -1\n",
    "            third = word[t]\n",
    "            if self.isConsonant(word, f) and self.isVowel(word, s) and self.isConsonant(word, t):\n",
    "                if third != 'w' and third != 'x' and third != 'y':\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    #function returns argument 'orig' with 'rem' replaced by 'rep' if present\n",
    "    def replace(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        replaced = base + rep\n",
    "        return replaced\n",
    "\n",
    "    #function returns argument 'orig' with 'rem' replaced by 'rep' if present and m>0\n",
    "    def replaceM0(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 0:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    #function returns argument 'orig' with 'rem' replaced by 'rep' if present and m>1    \n",
    "    def replaceM1(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 1:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    #Step 1a    \n",
    "    def step1a(self, word):\n",
    "        if word.endswith('sses'):\n",
    "            word = self.replace(word, 'sses', 'ss')\n",
    "        elif word.endswith('ies'):\n",
    "            word = self.replace(word, 'ies', 'i')\n",
    "        elif word.endswith('ss'):\n",
    "            word = self.replace(word, 'ss', 'ss')\n",
    "        elif word.endswith('s'):\n",
    "            word = self.replace(word, 's', '')\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "    \n",
    "    #Step 1b\n",
    "    def step1b(self, word):\n",
    "        flag = False\n",
    "        if word.endswith('eed'):\n",
    "            result = word.rfind('eed')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 0:\n",
    "                word = base\n",
    "                word += 'ee'\n",
    "        elif word.endswith('ed'):\n",
    "            result = word.rfind('ed')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        elif word.endswith('ing'):\n",
    "            result = word.rfind('ing')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        if flag:\n",
    "            if word.endswith('at') or word.endswith('bl') or word.endswith('iz'):\n",
    "                word += 'e'\n",
    "            elif self.doubleCons(word) and not self.endsWith(word, 'l') and not self.endsWith(word, 's') and not self.endsWith(word, 'z'):\n",
    "                word = word[:-1]\n",
    "            elif self.getM(word) == 1 and self.cvc(word):\n",
    "                word += 'e'\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    #Step 1c\n",
    "    def step1c(self, word):\n",
    "        if word.endswith('y'):\n",
    "            result = word.rfind('y')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                word += 'i'\n",
    "        return word\n",
    "\n",
    "    #Step 2\n",
    "    def step2(self, word):\n",
    "        if word.endswith('ational'):\n",
    "            word = self.replaceM0(word, 'ational', 'ate')\n",
    "        elif word.endswith('tional'):\n",
    "            word = self.replaceM0(word, 'tional', 'tion')\n",
    "        elif word.endswith('enci'):\n",
    "            word = self.replaceM0(word, 'enci', 'ence')\n",
    "        elif word.endswith('anci'):\n",
    "            word = self.replaceM0(word, 'anci', 'ance')\n",
    "        elif word.endswith('izer'):\n",
    "            word = self.replaceM0(word, 'izer', 'ize')\n",
    "        elif word.endswith('abli'):\n",
    "            word = self.replaceM0(word, 'abli', 'able')\n",
    "        elif word.endswith('alli'):\n",
    "            word = self.replaceM0(word, 'alli', 'al')\n",
    "        elif word.endswith('entli'):\n",
    "            word = self.replaceM0(word, 'entli', 'ent')\n",
    "        elif word.endswith('eli'):\n",
    "            word = self.replaceM0(word, 'eli', 'e')\n",
    "        elif word.endswith('ousli'):\n",
    "            word = self.replaceM0(word, 'ousli', 'ous')\n",
    "        elif word.endswith('ization'):\n",
    "            word = self.replaceM0(word, 'ization', 'ize')\n",
    "        elif word.endswith('ation'):\n",
    "            word = self.replaceM0(word, 'ation', 'ate')\n",
    "        elif word.endswith('ator'):\n",
    "            word = self.replaceM0(word, 'ator', 'ate')\n",
    "        elif word.endswith('alism'):\n",
    "            word = self.replaceM0(word, 'alism', 'al')\n",
    "        elif word.endswith('iveness'):\n",
    "            word = self.replaceM0(word, 'iveness', 'ive')\n",
    "        elif word.endswith('fulness'):\n",
    "            word = self.replaceM0(word, 'fulness', 'ful')\n",
    "        elif word.endswith('ousness'):\n",
    "            word = self.replaceM0(word, 'ousness', 'ous')\n",
    "        elif word.endswith('aliti'):\n",
    "            word = self.replaceM0(word, 'aliti', 'al')\n",
    "        elif word.endswith('iviti'):\n",
    "            word = self.replaceM0(word, 'iviti', 'ive')\n",
    "        elif word.endswith('biliti'):\n",
    "            word = self.replaceM0(word, 'biliti', 'ble')\n",
    "        return word\n",
    "\n",
    "    #Step 3\n",
    "    def step3(self, word):\n",
    "        if word.endswith('icate'):\n",
    "            word = self.replaceM0(word, 'icate', 'ic')\n",
    "        elif word.endswith('ative'):\n",
    "            word = self.replaceM0(word, 'ative', '')\n",
    "        elif word.endswith('alize'):\n",
    "            word = self.replaceM0(word, 'alize', 'al')\n",
    "        elif word.endswith('iciti'):\n",
    "            word = self.replaceM0(word, 'iciti', 'ic')\n",
    "        elif word.endswith('ful'):\n",
    "            word = self.replaceM0(word, 'ful', '')\n",
    "        elif word.endswith('ness'):\n",
    "            word = self.replaceM0(word, 'ness', '')\n",
    "        return word\n",
    "\n",
    "    #Step 4\n",
    "    def step4(self, word):\n",
    "        if word.endswith('al'):\n",
    "            word = self.replaceM1(word, 'al', '')\n",
    "        elif word.endswith('ance'):\n",
    "            word = self.replaceM1(word, 'ance', '')\n",
    "        elif word.endswith('ence'):\n",
    "            word = self.replaceM1(word, 'ence', '')\n",
    "        elif word.endswith('er'):\n",
    "            word = self.replaceM1(word, 'er', '')\n",
    "        elif word.endswith('ic'):\n",
    "            word = self.replaceM1(word, 'ic', '')\n",
    "        elif word.endswith('able'):\n",
    "            word = self.replaceM1(word, 'able', '')\n",
    "        elif word.endswith('ible'):\n",
    "            word = self.replaceM1(word, 'ible', '')\n",
    "        elif word.endswith('ant'):\n",
    "            word = self.replaceM1(word, 'ant', '')\n",
    "        elif word.endswith('ement'):\n",
    "            word = self.replaceM1(word, 'ement', '')\n",
    "        elif word.endswith('ment'):\n",
    "            word = self.replaceM1(word, 'ment', '')\n",
    "        elif word.endswith('ent'):\n",
    "            word = self.replaceM1(word, 'ent', '')\n",
    "        elif word.endswith('ou'):\n",
    "            word = self.replaceM1(word, 'ou', '')\n",
    "        elif word.endswith('ism'):\n",
    "            word = self.replaceM1(word, 'ism', '')\n",
    "        elif word.endswith('ate'):\n",
    "            word = self.replaceM1(word, 'ate', '')\n",
    "        elif word.endswith('iti'):\n",
    "            word = self.replaceM1(word, 'iti', '')\n",
    "        elif word.endswith('ous'):\n",
    "            word = self.replaceM1(word, 'ous', '')\n",
    "        elif word.endswith('ive'):\n",
    "            word = self.replaceM1(word, 'ive', '')\n",
    "        elif word.endswith('ize'):\n",
    "            word = self.replaceM1(word, 'ize', '')\n",
    "        elif word.endswith('ion'):\n",
    "            result = word.rfind('ion')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 1 and (self.endsWith(base, 's') or self.endsWith(base, 't')):\n",
    "                word = base\n",
    "            word = self.replaceM1(word, '', '')\n",
    "        return word\n",
    "\n",
    "    #Step 5a\n",
    "    def step5a(self, word):\n",
    "        if word.endswith('e'):\n",
    "            base = word[:-1]\n",
    "            if self.getM(base) > 1:\n",
    "                word = base\n",
    "            elif self.getM(base) == 1 and not self.cvc(base):\n",
    "                word = base\n",
    "        return word\n",
    "\n",
    "    #Step 5b\n",
    "    def step5b(self, word):\n",
    "        if self.getM(word) > 1 and self.doubleCons(word) and self.endsWith(word, 'l'):\n",
    "            word = word[:-1]\n",
    "        return word\n",
    "\n",
    "    # function returns word after stemming\n",
    "    def stem(self, word):\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5a(word)\n",
    "        word = self.step5b(word)\n",
    "        return word   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FINAL OUTPUT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natur Languag Process Domain evol past year . We got seen breakthroughs- ULMFiT, ELMo, BERT, . These vastli contribut State-of-the-art research NLP . current, We abl predict succeed sentenc, given sequenc preced word . downstream NLP task depen heavili pre-train word embed, abil enhanc lear gener inform lear untagged/unlabel inform, rel eas includ lear methodologi . Sever recent propos approach transcend initi “ word, embed ” paradigm better model addit featur subword structur mean ambigu . Even advantag, embed disadvantag ’ accustom simpli initi embed layer neural network requir specif rewor gener model architectur . The challeng rais differ word embed effect address FLAIR . Develop & open-sour Zalando Research; FLAIR simpl natur languag process (NLP) library/framework . PyTorch, buil block FLAIR ’ framework, renow deep lear framework . framework provid singl, simpl unifi interfac word document embed conceptu differ . The interfac effect hide embedding-specif engin complex permit research “ mix match ” embed littl effort produc state art result . The framework addition implement customari model train hyperparamet select routin . One interes compon FLAIR inbuilt data fetc modul . The modul access publicli avail dataset mold data structur experi eas . The framework suppli pre-train model follow NLP task: Name-Ent Recognit (NER), Parts-of-Speech Tag (PoS), Text Classif, sens disambigu classif, Train Custom Model . FLAIR run python minimum version requir python 3.6 environ . The librari instal pip, command: pip instal flair . All appear promis set FLAIR apart outperfor state-of-the-art result NLP includ Allen AI, Bi-direct LSTM . What provid FLAIR edg? The solut li featur embed FLAIR librari . It contain popular state-of-the-art word embed, GloVe, BERT, ELMo, Charact Embed, etc . FLAIR API enab us great eas . variant word embed combin singl embed model em dataset latest FLAIR interfac . Thi methodologi combin embed enhan tradit benchmark NLP produc signific rise F1 score major NLP task . Apart individu embed combin embed, FLAIR provid signatur embed call ‘ FLAIR Embed ’ . The FLAIR embed character uniqu contextu string embed . Context plai major role predic charact knowledg lear previou charact form cornerston sequenc model . Contextu String Embed leverag intern state train charact languag model produc novel type word embed . In simpl term, us certain intern princip train charact model, word differ mean differ sentenc . FLAIR contextu representations/embed hold knowledg syntact semant inform improv downstream NLP task . FLAIR set clear benchmark word embed stac word embed . These implemen hassl high-level API . futur bright FLAIR embed continu produc stride NLP domain.\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "assignment_v3=[]\n",
    "for word in assignment_v1:\n",
    "    assignment_v3.append(ps.stem(word))\n",
    "    \n",
    "assignment_v2 = TreebankWordDetokenizer().detokenize(assignment_v3)\n",
    "print (assignment_v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
